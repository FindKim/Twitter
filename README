In order to run our code and to see the data, you must be on a machine in the ND cluster which has hadoop set up on it. disc01.crc.nd.edu is one such machine.
To run the jobs we have written, you also must have the correct environment variables set. Depending on your configuration file, these may already be set, but to be safe you can run ./setHadoop (for csh). If you are in a different shell, you can read setHadoop to see which variables to set.

We have a four stage map reduce process. The results of the first of these stages can be compared to a sequential program written to achieve the same result.
The results of the first and third stages can be combined to create a nice visualization of the data.
The results of the fourth stage stand alone as an interesting set.

All stages are accomplished by follow the same form (\ represents that the line continues):

hadoop jar $HADOOP_HOME/mapred/contrib/streaming/hadoop-*-streaming.jar \
-input "/hadoop/path/to/input/input.txt" \
-output /hadoop/path/to/output_directory \
-mapper mapper.py \
-file mapper.py \
-reducer reducer.py \
-file reducer.py

Rounds 3 and 4 require additional flags that are explained in turn.

The input file is an actual text file in Hadoop, and the output file is a directory which will contain two files upon completion, SUCCESS and part-00000.
The results are in the part-00000 file, and the next stage requires /hadoop/path/to/output_directory/part-00000.

The first stage uses a mapper that is called "tweet_mapper_{DATASET}.py" where the mapper is specific to the dataset. The reducer is tweet_reducer.py.
This produces the results to the second stage, is a list of tab delimited triples of <hashtag, timebin, count>.

The second stage uses assisted_wordcount_[mapper|reducer].pl as the mapper and reducer respectively. It takes the output of the first stage (in /path/to/output/part-00000) and produces a file listing <hashtag, totalCount>.

The third stage uses ordering_[mapper|reducer].pl to sort the file created by round 2 of the map reduce, and needs flags to the suffle sort phase because of its end goal. Before the -input in the hadoop command, the following are also needed immediately after the hadoop jar line to sort descending by numeric keys:

hadoop jar $HADOOP_HOME/mapred/contrib/streaming/hadoop-*-streaming.jar \
-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \
-D  mapred.text.key.comparator.options=-nr \
-input "/hadoop/path/to/input/input.txt" \

This third round makes it easy to see which hashtags were the most popular throughout the entire collection period, and allows for an easy way to use the top, middle, or bottom parts (in terms of number of times a hashtag was tweeted) for studying hashtags. We use the top ten files to see the more interesting histograms, which we'll talk about shortly.

The fourth stage uses length_study_[mapper|reducer].pl to produce a file with pairs of <hashtag_length, avg_tweets>. It, like round 3, requires the two -D options, but the second one need only by "...options=-n", it does not require the r. Round 4 takes an entire output file (part-00000 of the output directory) of a third round map reduce and produces a hashtag length to average number of occurrences per hashtag of that length. This gives insight into how the length of a hashtag might affect the number of times it occurs within a tweet.

Create a graph:
The output of the first MapReduce that produces the file listing <hashtag, timestamp/tcount> and the output from the third MapReduce that produces the file listing <hashtag, total_count> can be used to create gnuplot plots to visualize the lifecycle of a certain hashtag. To do so, you would need to extract all the occurences of the hashtag of choice along with the timestamp and count into a separate file. We did this with a python script "top10_hashtag_files_to_plot.py" that creates a file for the top 10 hashtags found with all the occurences for the respective hashtags in the file.
	Usage: ./top10_hashtag_files_to_plot.py [hashtag time stamp file] [hashtag total count file]
The script parses out all the occurences of a top hashtag from the file with timestamp and count into another file, [#hashtag].txt.
	This file can then be used by the gnuplot script. plot_hashtags_sample.gp is a sample gnuplot script that plots the number of occurences the hashtag of interest is tweeted over time. 
	Usage: gnuplot plot_hashtags_sample.gp
	Usage: ps2pdf *.ps
	Small tweaks are needed in the plot_hashtags_sample.gp script. The output name and the file to be plotted. Specific instructions where to edit are commented in the gnuplot script. "sample_plot.pdf" is a sample plot that the current gnuplot script outputs.
	
To get information on sequential performance and to see it run, look under /users/kngo/Twitter in hadoop for the [event]_tweets_files "directory." 'hadoop get' the appropriate files to your local space. Type make to compile sequential.cpp into the executable sequential via the available makefile. sequential reads from stdin and thus can then be invoked by:

cat inputFile | ./sequential outputFile timeBinWidth

We always used timeBinWidth = 10.
